{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from scipy.stats import entropy\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "import gc\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr_torch.models import *\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "import torch\n",
    "from tqdm import tqdm as tq\n",
    "\n",
    "df = reduce_mem(df)\n",
    "\n",
    "\n",
    "def reduce_mem(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = joblib.load(\"labels.pkl\")# train label\n",
    "df = joblib.load(\"df.pkl\")        # train,test concat\n",
    "train_num = joblib.load(\"train_num.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 融合gbdt，input->多个神经元->cancat->batch_Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_oofLIST = []\n",
    "oof_cnt = 0\n",
    "import os\n",
    "test_num = len(df) - train_num\n",
    "for i in os.listdir(\"stacking/stacking/\"):\n",
    "    if i[-3:] != \"pkl\":\n",
    "        continue\n",
    "    pred = pd.read_pickle(\"stacking/stacking/\"+i)\n",
    "    oof = pred[\"oof\"]\n",
    "    testpred = pred[\"pred\"]\n",
    "    if len(oof) == train_num:\n",
    "        oof_cnt += 1\n",
    "        oof = list(oof) + list(testpred)\n",
    "        df[\"oof_\"+str(oof_cnt)] = oof\n",
    "        meta_oofLIST.append(\"oof_\"+str(oof_cnt))\n",
    "print(meta_oofLIST)\n",
    "\n",
    "# 读取zhangqi的oof，下面的模型没用到，即下面是一个单模型，显然也可以融合进去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 融合deviceid_newsid_deepwalk_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "deepwalk = pd.read_pickle(\"deviceid_newsid_deepwalk_8.pkl\")\n",
    "\n",
    "t1 = pd.read_csv('dataset/train.csv')\n",
    "t2 = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "DEVICE = list(t1[\"deviceid\"]) + list(t2[\"deviceid\"])\n",
    "\n",
    "df[\"deviceid\"] = DEVICE\n",
    "del t1,t2\n",
    "df = df.merge(deepwalk,how='left',on='deviceid')\n",
    "\n",
    "DFCOLUMNS = list(df.columns)\n",
    "deepwalk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计各类特征的名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import *\n",
    "\n",
    "exposure_ts_gapLIST = [i for i in DFCOLUMNS if i[-3:]=='gap']\n",
    "exposure_ts_gapLIST\n",
    "countLIST = [i for i in DFCOLUMNS if 'count' in i]\n",
    "countLIST\n",
    "categoryLIST = [\n",
    "    'deviceid', 'newsid', 'pos', 'app_version', 'device_vendor',\n",
    "    'netmodel', 'osversion', 'device_version', 'lng', 'lat', 'lng_lat',\n",
    "    'day','hour','minute'\n",
    "]\n",
    "embLIST = [i for i in DFCOLUMNS if 'emb' in i]\n",
    "embLIST\n",
    "nuniqueLIST = [i for i in DFCOLUMNS if 'nunique' in i]\n",
    "nuniqueLIST\n",
    "\n",
    "\n",
    "ctrLIST = [i for i in DFCOLUMNS if 'ctr'in i]\n",
    "\n",
    "otherLIST =  [i for i in DFCOLUMNS if i not in exposure_ts_gapLIST and\\\n",
    "              i not in countLIST and i not in embLIST and i not in nuniqueLIST\\\n",
    "             and i not in categoryLIST and i not in ctrLIST]\n",
    "otherLIST\n",
    "\n",
    "\n",
    "# small_exposure_ts_gapLIST = defaultdict(list)\n",
    "# for i in exposure_ts_gapLIST:\n",
    "#     t = i.split(\"exposur\")[0][:-2]\n",
    "#     small_exposure_ts_gapLIST[t].append(t)\n",
    "#     print(t)\n",
    "\n",
    "\n",
    "LISTmap = {\n",
    "    \"exposure_ts_gapLIST\":exposure_ts_gapLIST,\n",
    "    \"countLIST\":countLIST,\n",
    "    \"otherLIST\":otherLIST,\n",
    "    \"embLIST\":embLIST,\n",
    "    \"categoryLIST\":categoryLIST,\n",
    "    \"nuniqueLIST\":nuniqueLIST,\n",
    "#     \"meta_oofLIST\":meta_oofLIST\n",
    "}\n",
    "\n",
    "\n",
    "sparse_features = categoryLIST\n",
    "dense_features = []\n",
    "for i in LISTmap:\n",
    "    if i != \"categoryLIST\" :#and i != \"countLIST\":\n",
    "        dense_features += LISTmap[i]\n",
    "print(len(dense_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取各组指标的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'gap' in \"lng_lat_next2_exposure_ts_gap\"\n",
    "# print(meta_oofLIST)\n",
    "train_num == len(labels)\n",
    "from collections import *\n",
    "featureNum = []\n",
    "\n",
    "indexMap = defaultdict(list)\n",
    "for index,i in enumerate(dense_features):\n",
    "    for j in LISTmap:\n",
    "        if j == \"categoryLIST\":\n",
    "            continue\n",
    "        if i in LISTmap[j]:\n",
    "            indexMap[j].append(index)\n",
    "input_dim = 0\n",
    "\n",
    "for i in indexMap:\n",
    "    print(i,len(indexMap[i]))\n",
    "    if i != \"meta_oofLIST\":\n",
    "        featureNum.append(len(indexMap[i]))\n",
    "        input_dim += len(indexMap[i])\n",
    "print(input_dim)\n",
    "featureNum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sampleRate = 1 \n",
    "\n",
    "\n",
    "# 过滤异常值，有些值太大时，归一化后，使得数据整体偏小，不利于NN学习\n",
    "\n",
    "for i in LISTmap[\"exposure_ts_gapLIST\"]:\n",
    "    q = df[i].quantile(0.5)\n",
    "    print(i,q)\n",
    "    df.loc[df[i]>q,i] = q\n",
    "\n",
    "df[dense_features] = df[dense_features].fillna(0)\n",
    "\n",
    "dfmin = np.min(df[dense_features])\n",
    "dfmax = np.max(df[dense_features])\n",
    "\n",
    "dfdiff = dfmax - dfmin \n",
    "dfdiff[dfdiff==0] = 1\n",
    "\n",
    "\n",
    "print(\"计算min、max完成\")\n",
    "\n",
    "data = df[:train_num][::sampleRate]\n",
    "test = df[train_num:][::sampleRate]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target = ['target']\n",
    "\n",
    "data[\"target\"] = labels[:train_num][::sampleRate]\n",
    "test[\"target\"] = [0]*len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "        \n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs\n",
    "class groupy_NN(nn.Module):\n",
    "    def __init__(self ,input_dim ,hidden_dim, dropout = 0.75):\n",
    "        super(groupy_NN, self).__init__()\n",
    "        \n",
    "        self.inpt_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.concat_dim = input_dim * 2 #+ len(indexMap[\"meta_oofLIST\"]) * 10\n",
    "        \n",
    "        self.sub_meta = nn.Sequential(nn.Linear(1,10),nn.ReLU(),nn.Linear(10,10),nn.ReLU())\n",
    "        self.subfc0 = nn.Sequential(nn.Linear(featureNum[0],featureNum[0]*2),nn.ReLU(),nn.Linear(featureNum[0]*2,featureNum[0]*2),nn.ReLU())\n",
    "        self.subfc1 = nn.Sequential(nn.Linear(featureNum[1],featureNum[1]*2),nn.ReLU(),nn.Linear(featureNum[1]*2,featureNum[1]*2),nn.ReLU())\n",
    "        self.subfc2 = nn.Sequential(nn.Linear(featureNum[2],featureNum[2]*2),nn.ReLU(),nn.Linear(featureNum[2]*2,featureNum[2]*2),nn.ReLU())\n",
    "        self.subfc3 = nn.Sequential(nn.Linear(featureNum[3],featureNum[3]*2),nn.ReLU(),nn.Linear(featureNum[3]*2,featureNum[3]*2),nn.ReLU())\n",
    "        self.subfc4 = nn.Sequential(nn.Linear(featureNum[4],featureNum[4]*2),nn.ReLU(),nn.Linear(featureNum[4]*2,featureNum[4]*2),nn.ReLU())\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(self.concat_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)\n",
    "        self.fc_tmp = nn.Sequential(nn.Linear(hidden_dim//2, hidden_dim//2),nn.ReLU())\n",
    "        self.fc3 = nn.Linear(hidden_dim//2, 1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(self.concat_dim)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(int(hidden_dim/4))\n",
    "        self.bn4 = nn.BatchNorm1d(int(hidden_dim/8))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b_size = x.size(0)\n",
    "#         print(x.size)\n",
    "        groupyX = []\n",
    "        for i in indexMap:\n",
    "            if i != \"meta_oofLIST\":\n",
    "                groupyX.append(x[:,indexMap[i]])\n",
    "    \n",
    "        meta_X = []\n",
    "#         print(len(indexMap[\"meta_oofLIST\"]))\n",
    "#         for i in indexMap[\"meta_oofLIST\"]:\n",
    "#             meta_X.append(x[:,[i]])\n",
    "\n",
    "        embX = []\n",
    "        for i in meta_X:\n",
    "            embX.append(self.sub_meta(i))\n",
    "    \n",
    "#         embX_sub = torch.cat(embX,axis=1)\n",
    "#         print(embX_sub.size())\n",
    "        selfFCLIST = [self.subfc0,self.subfc1,self.subfc2,self.subfc3,self.subfc4]\n",
    "        \n",
    "        for index,i in enumerate(selfFCLIST):\n",
    "            embX.append(selfFCLIST[index](groupyX[index]))\n",
    "        \n",
    "        \n",
    "        out_emb = torch.cat(embX,axis=1)\n",
    "        out_emb = self.bn1(out_emb)\n",
    "        \n",
    "#         x = x.view(-1, 1)\n",
    "        y = self.fc1(out_emb)\n",
    "        y = self.relu(y)\n",
    "        y = self.bn2(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc_tmp(y)\n",
    "        out= self.fc3(y)\n",
    "        return out\n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        pass\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        label = self.df.loc[i,\"target\"]\n",
    "        data = self.df.loc[i,dense_features].copy()# - dfmin)/dfdiff\n",
    "        data = (data - dfmin) / dfdiff\n",
    "        return torch.tensor(data,dtype=torch.float32).cuda(),\\\n",
    "    torch.tensor([label], dtype=torch.float32).cuda()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[dense_features].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.describe()\n",
    "len(dense_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train val split()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "batch_size = 1024 * 4\n",
    "test_size = batch_size \n",
    "Test = TorchDataset(test.reset_index())\n",
    "test_loader = torch.utils.data.DataLoader(Test, batch_size=test_size, shuffle=False)\n",
    "\n",
    "print(\"len(test_loader):\",len(test_loader))\n",
    "\n",
    "\n",
    "# train, val = train_test_split(data.iloc[::10], test_size=0.2)\n",
    "\n",
    "# Train = data[data[\"day\"]<10].iloc[::10]\n",
    "val = data[data[\"day\"]==10].iloc[::10]\n",
    "Train = data.iloc[::5]\n",
    "\n",
    "train = TorchDataset(Train.reset_index())\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"len(train_loader):\",len(train_loader))\n",
    "\n",
    "\n",
    "Val = TorchDataset(val.reset_index())\n",
    "val_loader = torch.utils.data.DataLoader(Val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "model = groupy_NN(input_dim,1000)\n",
    "\n",
    "\n",
    "test_preds = np.zeros((len(test)))\n",
    "\n",
    "\n",
    "print(model)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4,weight_decay=1e-5) # Using Adam optimizer\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"_______________________\",epoch,\"_________________________\")\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    #avg_auc = 0.\n",
    "    test_preds_fold = np.zeros((len(test)))\n",
    "    valid_preds_fold = np.zeros((len(val)))\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(\"BATCH_{}_loss_________________________{}\".format(i,round(loss.item(),4)))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()/len(train_loader)\n",
    "    model.eval()\n",
    "    avg_val_loss = 0.\n",
    "    for i, (x_batch, y_batch) in enumerate(val_loader):\n",
    "        \n",
    "        y_pred = model(x_batch).detach()\n",
    "        if i%20 == 0:\n",
    "            print(sigmoid(y_pred.cpu().numpy())[:,0])\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(val_loader)\n",
    "        valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            \n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "        epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "    auc = round(roc_auc_score(val[\"target\"].values,valid_preds_fold),4)\n",
    "    print(auc)\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "print(\"test\")\n",
    "for i, (x_batch,_) in enumerate(test_loader):\n",
    "    y_pred = model(x_batch).detach()\n",
    "#         print(x_batch.size())\n",
    "    test_preds_fold[i * test_size:(i+1) * test_size] = sigmoid(y_pred.cpu().numpy())[:,0]\n",
    "\n",
    "    if i % 20==0:\n",
    "        print(\"test______\",i)\n",
    "        print(sigmoid(y_pred.cpu().numpy())[:,0])\n",
    "#             print(y_pred.cpu().numpy().shape)\n",
    "#     print(auc)\n",
    "test_preds += test_preds_fold \n",
    "\n",
    "\n",
    "sub = pd.read_csv('dataset/sample.csv')\n",
    "sub[\"target\"] = test_preds_fold\n",
    "print('=============================================== sub save ===============================================')\n",
    "sub.to_csv('sub_prob_{}_{}_{}.csv'.format(auc, best_f1, sub['target'].mean()), index=False)\n",
    "sub['target'] = sub['target'].apply(lambda x: 1 if x >= best_t else 0)\n",
    "sub.to_csv('sub_{}_{}_{}.csv'.format(auc, best_f1, sub['target'].mean()), index=False)\n",
    "# print('runtime:', time.time() - t)\n",
    "print('finish.')\n",
    "print('========================================================================================================')\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
